#+title: Lab2
#+PROPERTY: header-args:python   :session :exports both :eval no-export :async
* Lab 3: A simple lexer
- Course :: Formal Languages & Finite Automata
- Author :: Balan Artiom

* Theory
**Tokens** within parsers correspond to **terminal symbols** within formal grammars.

A grammar defines how a list of /terminal symbols/ is derived from the starting non-terminal symbol.
Terminal symbols are like building blocks, which can be parsed into a tree to reason about their meaning.
Problem is, we want to write our programs as text, and not as a stream of abstract symbols.
We want text editors, not Sctratch-like GUIs.

Why can't we just represent terminals as strings, you ask?
Because for complex programming languages,
splitting a stream of text (e.g. a source code file) into the terminals that we meant is not trivial.

** Plus-equal or plus and equal?
For example, say we have this grammar, where strings surrounded by quotes are terminal symbols.
#+begin_example
increment -> ID '+=' NUM
add -> ID '=' NUM '+' NUM
ID -> [a..z]+
NUM -> [0..9]+
#+end_example

Then, we write a short program according to this grammar:
#+begin_example
a=2+2
a+=1
#+end_example

How do you split this program into its corresponding terminals?
The first line is pretty easy:
#+begin_example
'a' '=' '2' '+' '2'
#+end_example

But the second line could be split in different ways:
#+begin_example
'a' '+' '=' '1'
#+end_example
Or
#+begin_example
'a' '+=' '1'
#+end_example

Obviously, we meant the second meaning,
but the rules of splitting a string into its corresponding terminals are not represented in the grammar,

This problem could be generalized as "should I pick the longer string or the shorter one?".
** Keyword or identifier?
Another problem that arises when we want to translate a text into terminals
is that sometimes a string can be interpreted as different terminals.
The most common instance of this is when keywords could be interpreted as identifiers.
For example, =break= could be either a =KEYWORD= or an =IDENTIFIER=,
since this is how an =IDENTIFIER= is usually defined:
#+begin_example
IDENTIFIER -> letter (letter|number|underscore)*
#+end_example
** Comments
Yet another problem is ignoring comments inside source code.

Since comments can appear anywhere in your program (e.g. in C),
you would have to mention them everywhere in your grammar.

For example:
#+begin_example
increment -> COMMENT* ID COMMENT* '+=' COMMENT* NUM COMMENT*
add -> COMMENT* ID COMMENT* '=' COMMENT* NUM COMMENT* '+' COMMENT* NUM COMMENT*
ID -> [a..z]+
NUM -> [0..9]+
COMMENT -> '/*' [^*] '*/'
#+end_example

Hopefully you get the point, this is not at all practical.
** Solution: tokens
It's clear by now that representing a terminal as a string is not viable.
The solution is to represent terminal symbols with something more abstract than strings of characters,
something unambiguous, and this something is /tokens/,

Now that we represent terminals as tokens,
we need a separate program that would translate a stream of text into these tokens.
This program is called a "lexer", or "tokenizer", or "scanner", or whatever.

A tokenizer is naturally very ugly:
it has to handle a lot of exceptions and edge-cases:
- intertwined comments
- keyword vs. identifier
- translate the longer sub-string or the shorter?
- indentation-based blocks (off-side rule)

By delegating this task to the tokenizer,
the parser doesn't have to worry about these ugly hacks,
all it sees is a list of tokens
which can be elegantly parsed according to the grammar rules alone.

Tokens are also convenient because they can hold useful information about each lexeme.
A token is usually represented as a pair of a type (usually =enum= value) and a value,
which could be of any type, even a data structure holding multiple values:
- token value
- location in the text stream

Let's see how a tokenizer would solve the previous example.
Instead of representing terminals as strings,
we represent them as token names:
#+begin_example
increment -> ID PLUSEQ NUM
add -> ID EQ NUM PLUS NUM
#+end_example

Then, a tokenizer for this language would have the role of translating that program into the following tokens:
#+begin_example
ID(a) EQ NUM(2) PLUS NUM(2)
ID(a) PLUSEQ NUM(1)
#+end_example

I used the notation =TOKENNAME(value)= to represent a token and its value.

As for the second problem, a tokenizer usually solves this by prioritizing keywords over identifiers.

Ignoring comments in a tokenizer is also pretty straight-forward.
* Objectives
- [X] Implement a lexer and show how it works.
* Results
I wrote a lexer for python-like syntax, hence, all the example strings are valid python code.

#+begin_src python :exports none
import sys, os
sys.path.append(os.path.join(os.path.dirname(sys.argv[0]), '..', 'src'))
from lexer import *

def tabulate_tokens(s):
    ls = get_tokens(inp)
    from tabulate import tabulate
    return tabulate([("={}=".format(t.type), "={}=".format(t.value) if t.value else '') for t in ls], tablefmt="orgtbl", headers=["Token name", "Token value"])
#+end_src

#+RESULTS:

Let's parse a simple variable assignment:
#+name: input
#+begin_src python  :eval no
a_1 += 12 * 3 + 2
#+end_src

#+RESULTS: input
: /tmp/babel-P0bk7B/python-M2mdv0

Each token is represented by two things: a name and an optional value.
In this example, notice that the token for the variable =a= is of type =ID=,
which stands for "identifier", and the token value is the name of the variable.

Similarly, numbers are represented by =NUMBER= tokens, with their value as the token value.
#+begin_src python :var inp=(get-val-of-named-src-block "input") :exports results :results drawer
tabulate_tokens(inp)
#+end_src

#+RESULTS:
:results:
| Token name            | Token value |
|-----------------------+-------------|
| =TokenType.ID=        | =a_1=       |
| =TokenType.DELIMITER= | =+==        |
| =TokenType.NUMBER=    | =12=        |
| =TokenType.OPERATOR=  | =*=         |
| =TokenType.NUMBER=    | =3=         |
| =TokenType.OPERATOR=  | =+=         |
| =TokenType.NUMBER=    | =2=         |
| =TokenType.EOF=       |             |
:end:

Now let's see how a lexer recognizes indentation:
#+name: inp2
#+begin_src python :eval no
def t(arg):
    print(arg)
#+end_src

#+RESULTS: inp2

#+begin_src python :var inp=(get-val-of-named-src-block "inp2") :exports results :results drawer
tabulate_tokens(inp)
#+end_src

#+RESULTS:
:results:
| Token name            | Token value |
|-----------------------+-------------|
| =TokenType.KEYWORD=   | =def=       |
| =TokenType.ID=        | =t=         |
| =TokenType.DELIMITER= | =(=         |
| =TokenType.ID=        | =arg=       |
| =TokenType.DELIMITER= | =)=         |
| =TokenType.DELIMITER= | =:=         |
| =TokenType.INDENT=    |             |
| =TokenType.ID=        | =print=     |
| =TokenType.DELIMITER= | =(=         |
| =TokenType.ID=        | =arg=       |
| =TokenType.DELIMITER= | =)=         |
| =TokenType.DEDENT=    |             |
| =TokenType.EOF=       |             |
:end:

Did you catch that?
The lexer generated two additional "invisible" tokens
to let the parser know about the indented block: =INDENT= and =DEDENT=.

You could visualize the token placement like this:
#+begin_example
1. def t(arg):
     v INDENT
2.    print(arg)
3.
  ^ DEDENT
#+end_example

Let's see a more complicated example:
#+name: inp3
#+begin_src python :eval no
if a:
    if b:
        foo()
bar()
#+end_src

#+begin_src python :var inp=(get-val-of-named-src-block "inp3") :exports results :results drawer
tabulate_tokens(inp)
#+end_src

#+RESULTS:
:results:
| Token name            | Token value |
|-----------------------+-------------|
| =TokenType.KEYWORD=   | =if=        |
| =TokenType.ID=        | =a=         |
| =TokenType.DELIMITER= | =:=         |
| =TokenType.INDENT=    |             |
| =TokenType.KEYWORD=   | =if=        |
| =TokenType.ID=        | =b=         |
| =TokenType.DELIMITER= | =:=         |
| =TokenType.INDENT=    |             |
| =TokenType.ID=        | =foo=       |
| =TokenType.DELIMITER= | =(=         |
| =TokenType.DELIMITER= | =)=         |
| =TokenType.DEDENT=    |             |
| =TokenType.DEDENT=    |             |
| =TokenType.ID=        | =bar=       |
| =TokenType.DELIMITER= | =(=         |
| =TokenType.DELIMITER= | =)=         |
| =TokenType.EOF=       |             |
:end:

Let's visualize this too:
#+begin_example
1. if a:
     v INDENT
2.    if b:
          v INDENT
3.         foo()
4. bar()
  ^ 2 x DEDENT
#+end_example

Notice how two =DEDENT= tokens were generated before =bar()=,
because we "closed" two indented blocks.

The lexer recognizes comments too and ignores them:
#+name: inp4
#+begin_src python :eval no
 # this line has a bad indent
def t(arg):
    print(arg)  # this comment is inline
#+end_src

#+begin_src python :var inp=(get-val-of-named-src-block "inp4") :exports results :results drawer
tabulate_tokens(inp)
#+end_src

#+RESULTS:
:results:
| Token name            | Token value |
|-----------------------+-------------|
| =TokenType.KEYWORD=   | =def=       |
| =TokenType.ID=        | =t=         |
| =TokenType.DELIMITER= | =(=         |
| =TokenType.ID=        | =arg=       |
| =TokenType.DELIMITER= | =)=         |
| =TokenType.DELIMITER= | =:=         |
| =TokenType.INDENT=    |             |
| =TokenType.ID=        | =print=     |
| =TokenType.DELIMITER= | =(=         |
| =TokenType.ID=        | =arg=       |
| =TokenType.DELIMITER= | =)=         |
| =TokenType.DEDENT=    |             |
| =TokenType.EOF=       |             |
:end:

Notice that the first line has a bad indent (first line can't be indented in python),
but since it's a comment, we can ignore this issue (one more edge-case to consider).

There's one type of indentation error that can be recognized by the lexer (and 3 others that can only be recognized by the parser),
and that's the "inconsistent dedent":
#+name: inp5
#+begin_src python :eval no
def foo(a):
    if a == 1:
        return 1
   return 0
#+end_src

The lexer simply raises an exception for this example.

Notice how some delimiters start like operators, and viceversa:
#+name: inp6
#+begin_src python :eval no
a += b == c
#+end_src

#+begin_src python :var inp=(get-val-of-named-src-block "inp6") :exports results :results drawer
tabulate_tokens(inp)
#+end_src

#+RESULTS:
:results:
| Token name            | Token value |
|-----------------------+-------------|
| =TokenType.ID=        | =a=         |
| =TokenType.DELIMITER= | =+==        |
| =TokenType.ID=        | =b=         |
| =TokenType.OPERATOR=  | ====        |
| =TokenType.ID=        | =c=         |
| =TokenType.EOF=       |             |
:end:

In this case, the operator ==== starts like the delimiter ===, and the delimiter =+== starts like the operator =+=.
I'm not sure what's the proper way to deal with this, so my code is a bit hacky.

* Implementation
Indentation handling is implemented as described in the [[https://docs.python.org/3/reference/lexical_analysis.html#indentation][python docs]].

The entire "lexer" is a single function =get_tokens(s) -> ls=
that takes a string to be tokenized, and returns a list of all the tokens.

Initially I tried wrapping the tokenizer inside a class, but it didn't make sense
and only made things more obscure and complicated.
I don't see why you would need to maintain the state of a lexer by reading tokens one by one,
when you could instead get all the tokens at once.
And if you don't need a state, there's no need for an object.

The =get_tokens= function reads characters using either =getch()=  or =peek()=,
depending on whether it wants to also consume the character.

The entire function is a loop that tokenizes the entire string,
until there's no more characters left, after which it generates the last token, =EOF=.
